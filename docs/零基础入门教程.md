# 🧠 ViT+分割网络 零基础入门教程

> 本文档面向完全没有深度学习基础的同学，从零开始讲解这个脑部MRI图像分割项目。

---

## 📚 第一章：什么是图像分割？

### 1.1 通俗理解

想象你在看一张脑部CT照片，医生需要找出里面的肿瘤区域。**图像分割** 就是让计算机自动帮你把肿瘤区域"圈"出来。

```
原始图像          →        分割结果
┌────────────┐         ┌────────────┐
│    🧠      │         │    🧠      │
│   ○ 肿瘤   │   AI    │   🔴 标记  │
│            │   →     │            │
└────────────┘         └────────────┘
```

### 1.2 技术定义

**语义分割（Semantic Segmentation）**：给图像中每个像素分配一个类别标签。

- 输入：一张 256×256 的MRI图像
- 输出：一张 256×256 的掩码图（每个像素标记是"肿瘤"还是"背景"）

---

## 📚 第二章：什么是ViT和Transformer？

### 2.1 为什么需要ViT？

传统的图像处理用 **CNN（卷积神经网络）**，它很擅长看"局部细节"（比如边缘、纹理）。

但医学图像中，肿瘤可能分布在图像各处，我们需要看"全局关系"。

**ViT（Vision Transformer）** 就是来解决这个问题的！

### 2.2 ViT的核心思想

```
┌─────────────────────────────────────────────┐
│           原始图像 (256×256)                 │
│  ┌──┬──┬──┬──┐                              │
│  │1 │2 │3 │4 │  → 切成16×16的小块(Patches)   │
│  ├──┼──┼──┼──┤                              │
│  │5 │6 │7 │8 │  → 每个小块变成一个"词"       │
│  ├──┼──┼──┼──┤                              │
│  │..│..│..│..│  → 用Transformer处理这些"词" │
│  └──┴──┴──┴──┘                              │
└─────────────────────────────────────────────┘
```

**简单来说**：ViT把图像当成"一篇文章"，每个小块是一个"单词"，然后用处理语言的方式来理解图像！

### 2.3 Transformer的自注意力

**自注意力（Self-Attention）**：让每个"单词"都能"看到"其他所有"单词"，从而理解全局关系。

```
Patch 1 ←→ Patch 5 ←→ Patch 12
   ↑         ↑          ↑
   └─────────┴──────────┘
        全局关联
```

---

## 📚 第三章：TransUNet架构解析

### 3.1 为什么选TransUNet？

我们的项目使用 **TransUNet**，它结合了两种技术的优点：

| 技术 | 优点 | 用途 |
|------|------|------|
| **Transformer (ViT)** | 看全局 | 理解肿瘤的整体位置 |
| **CNN** | 看细节 | 精确分割边界 |

### 3.2 架构图解

```
输入图像 (3, 256, 256)
        │
        ├───────────────────────────┐
        ▼                           ▼
┌───────────────┐           ┌───────────────┐
│  ViT 编码器    │           │  CNN 编码器    │
│  (全局特征)    │           │  (局部特征)    │
│               │           │               │
│  Transformer  │           │  Conv+Pool    │
│  Blocks ×12   │           │  ×4层         │
└───────┬───────┘           └───────┬───────┘
        │                           │
        ▼                           ▼
┌─────────────────────────────────────────────┐
│              特征融合 (Fusion)               │
│         全局 + 局部 = 更好的理解              │
└─────────────────────────────────────────────┘
        │
        ▼
┌───────────────────────────┐
│         解码器            │
│  上采样 + 跳跃连接         │
│  逐步恢复到原始尺寸        │
└───────────────────────────┘
        │
        ▼
输出掩码 (1, 256, 256)
```

### 3.3 跳跃连接（Skip Connection）

解码器在上采样时，会"回头去拿"编码器中间层的特征，这样可以保留更多细节。

```
编码器层1 ─────────────────→ 解码器层4
编码器层2 ─────────────────→ 解码器层3
编码器层3 ─────────────────→ 解码器层2
编码器层4 ─────────────────→ 解码器层1
```

---

## 📚 第四章：项目代码结构

### 4.1 文件说明

```
VIT+分割网络的图像分割/
│
├── src/                    # 源代码目录
│   ├── model.py           # 🔥 核心：TransUNet模型定义
│   ├── dataset.py         # 📊 数据加载和预处理
│   ├── losses.py          # 📉 损失函数（Dice + BCE）
│   ├── metrics.py         # 📈 评价指标（Dice Score, IoU）
│   ├── train.py           # 🚀 训练脚本
│   ├── inference.py       # 🔍 推理和可视化
│   └── generate_synthetic_data.py  # 🎲 生成测试数据
│
├── data/                   # 数据目录
│   └── synthetic_mri/     # 模拟的MRI数据
│
├── docs/                   # 参考文档
│   ├── ViT_Mission.md     # 任务说明
│   └── *.pdf              # 论文和研究文档
│
├── outputs/               # 输出结果
│   └── prediction_*.png   # 可视化结果图
│
└── README.md              # 项目说明
```

### 4.2 代码模块关系

```
┌──────────────────────────────────────────────────────┐
│                      train.py                        │
│                    (训练入口)                         │
└──────────────────────────────────────────────────────┘
        │               │               │
        ▼               ▼               ▼
┌──────────┐    ┌──────────┐    ┌──────────┐
│ model.py │    │dataset.py│    │losses.py │
│ (模型)   │    │ (数据)   │    │ (损失)   │
└──────────┘    └──────────┘    └──────────┘
        │
        ▼
┌──────────────────────────────────────────────────────┐
│                    inference.py                      │
│                   (推理可视化)                        │
└──────────────────────────────────────────────────────┘
```

---

## 📚 第五章：损失函数和评价指标

### 5.1 Dice Loss

**Dice系数** 衡量预测结果和真实标签的重叠程度：

```
           2 × |预测 ∩ 真实|
Dice = ─────────────────────
         |预测| + |真实|
```

- Dice = 1.0 → 完美重叠 ✅
- Dice = 0.0 → 完全不重叠 ❌

**Dice Loss = 1 - Dice**（越小越好）

### 5.2 BCE Loss（二元交叉熵）

逐像素计算预测概率和真实标签的差距。

### 5.3 为什么用组合损失？

```
总损失 = 0.5 × Dice Loss + 0.5 × BCE Loss
```

- **Dice Loss**：关注整体重叠
- **BCE Loss**：关注每个像素

两者结合，效果更好！

### 5.4 IoU（交并比）

```
         |预测 ∩ 真实|
IoU = ──────────────────
       |预测 ∪ 真实|
```

IoU越高，分割效果越好。

---

## 📚 第六章：快速上手

### 6.1 运行环境

```bash
# 安装依赖
pip install torch torchvision matplotlib pillow numpy
```

### 6.2 生成测试数据

```bash
cd src
python generate_synthetic_data.py --output_dir ../data/synthetic_mri --num_samples 100
```

### 6.3 训练模型

```bash
python train.py --data_dir ../data/synthetic_mri --epochs 50 --batch_size 4
```

训练过程会显示：
```
Epoch [  1/50] | Train Loss: 0.8234, Dice: 0.1766 | Val Dice: 0.2100
Epoch [  2/50] | Train Loss: 0.7123, Dice: 0.2877 | Val Dice: 0.3200
...
>>> 保存最佳模型 (Dice: 0.8500)
```

### 6.4 查看结果

```bash
python inference.py --checkpoint ../checkpoints/best_model.pth --data_dir ../data/synthetic_mri
```

这会生成对比图：**原图 | 真实标签 | 模型预测**

---

## 📚 第七章：核心代码讲解

### 7.1 模型定义（model.py）

```python
class TransUNet(nn.Module):
    def __init__(self, img_size=256, num_classes=1):
        super().__init__()
        
        # ViT编码器 - 提取全局特征
        self.vit_encoder = ViTEncoder(...)
        
        # CNN编码器 - 提取局部特征
        self.cnn_encoder = CNNEncoder(...)
        
        # 解码器 - 融合特征并输出
        self.decoder = Decoder(...)
    
    def forward(self, x):
        # 1. ViT提取全局特征
        vit_features = self.vit_encoder(x)
        
        # 2. CNN提取局部特征
        cnn_features, skip = self.cnn_encoder(x)
        
        # 3. 解码器融合并输出
        output = self.decoder(vit_features, cnn_features, skip)
        
        return output
```

### 7.2 训练循环（train.py 简化版）

```python
for epoch in range(epochs):
    # 训练阶段
    for images, masks in train_loader:
        # 1. 前向传播
        predictions = model(images)
        
        # 2. 计算损失
        loss = criterion(predictions, masks)
        
        # 3. 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # 验证阶段
    val_dice = validate(model, val_loader)
    
    # 保存最佳模型
    if val_dice > best_dice:
        torch.save(model.state_dict(), 'best_model.pth')
```

---

## 📚 第八章：常见问题

### Q1: 显存不足（CUDA out of memory）

**解决方案**：
1. 减小 `batch_size`（如从8改为2）
2. 使用轻量版模型：`--model transunet_lite`

### Q2: 训练效果不好（Dice很低）

**解决方案**：
1. 增加训练轮数 `--epochs 100`
2. 调整学习率 `--lr 5e-5`
3. 检查数据是否正确加载

### Q3: 如何用自己的数据？

准备数据格式：
```
my_data/
├── patient1.tif        # 图像
├── patient1_mask.tif   # 掩码（同名+_mask）
├── patient2.tif
├── patient2_mask.tif
└── ...
```

然后运行：
```bash
python train.py --data_dir ../my_data
```

---

## 🎓 总结

| 概念 | 一句话解释 |
|------|----------|
| **图像分割** | 给每个像素分类 |
| **ViT** | 把图像当文章理解，看全局 |
| **CNN** | 用卷积提取细节特征 |
| **TransUNet** | ViT + CNN 的混合体 |
| **Dice Loss** | 衡量预测和真实的重叠度 |
| **跳跃连接** | 保留细节信息不丢失 |

---

**🎉 恭喜你完成了零基础入门！现在你已经理解了这个项目的核心原理。**
